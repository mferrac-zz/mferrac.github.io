<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matheus Ferraciolli on Matheus Ferraciolli</title>
    <link>https://mferrac.github.io/</link>
    <description>Recent content in Matheus Ferraciolli on Matheus Ferraciolli</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Jun 2019 16:05:32 -0300</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sugarcane Yield</title>
      <link>https://mferrac.github.io/post/sugarcane-yield/</link>
      <pubDate>Mon, 24 Jun 2019 16:05:32 -0300</pubDate>
      
      <guid>https://mferrac.github.io/post/sugarcane-yield/</guid>
      <description>

&lt;h1 id=&#34;yield-modeling&#34;&gt;Yield modeling&lt;/h1&gt;

&lt;p&gt;The harvesting operation involves transportation of heavy agricultural machinery across great distances in the fields.
Effective management of field operations can improve adequate machine use planning, which, in turn can reduce costs related to fuel and workforce allocation.&lt;/p&gt;

&lt;p&gt;If we can more accurately predict how much will be harvested in each field, we can have a better planning of the whole operation. In this post I&amp;rsquo;ll discuss how we can use available data and moder machine learning algorthims to improve harvest prediction.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Please&lt;/em&gt; note that the data is not allowed to be shared, so I&amp;rsquo;ll just post this as a guideline of what we can usually consider when modelling yield and show some of the insights we had along the way.&lt;/p&gt;

&lt;p&gt;My first publication regarded yield modeling for sugarcane comparing
We used a dataset containing, for every field:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Production data for 4 diferent mills across 4 years of harvest&lt;/li&gt;
&lt;li&gt;Climate data acquired from satelites and crossed on field level using &lt;code&gt;shapefiles&lt;/code&gt; of every field&lt;/li&gt;
&lt;li&gt;Soil data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the end, we developed models to predict, on field level, how much will be harvested in that year.
Below, I&amp;rsquo;ll explain the steps taken to model a problem like this.
The same procedure may be applied to many other crops, given an available dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;/home/mferrac/data/sugarcane.csv&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each row represents data from a whole cicle.
 Each column (or &lt;strong&gt;attribute&lt;/strong&gt;) represents data from a whole cicle.
 The problem we aim to solve is &amp;ldquo;given a field had a cycle with given attributes, how much will be harvested there?&amp;rdquo;.
 There is the bigger problem where we might not have some of these atributes like temperature, rainfall, etc. in advance, but I&amp;rsquo;ll leave that for another post.&lt;/p&gt;

&lt;h2 id=&#34;available-attributes&#34;&gt;Available attributes&lt;/h2&gt;

&lt;p&gt;In total, there were 63 attributes available, divided in production, soil, climate and chemical input data.&lt;/p&gt;

&lt;h3 id=&#34;production-data&#34;&gt;Production data&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;field_id&lt;/code&gt; : ID&amp;rsquo;s not used for prediction&lt;/li&gt;
&lt;li&gt;&lt;code&gt;farmcod&lt;/code&gt;: Part of the ID&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test&lt;/code&gt;: &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt; if it is in the &lt;code&gt;test_set&lt;/code&gt; or not&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X&lt;/code&gt;: Longitude&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y&lt;/code&gt;: Latitude&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tonnes_harvested&lt;/code&gt;: Tonnes harvested in the field (what we aim to predict)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;prod_env&lt;/code&gt;: Grade (AB, C, D, E, etc.) related to the local conditions to produce sugarcane&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spac&lt;/code&gt;: Spacing betweeen rows&lt;/li&gt;
&lt;li&gt;&lt;code&gt;harvest_no&lt;/code&gt;: Number of harvests. As a semiperennial culture, sugarcane is harvested more than once. It is usually an important attribute.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fire_raw&lt;/code&gt;: Was fire used or not before harvesting&lt;/li&gt;
&lt;li&gt;&lt;code&gt;no_field&lt;/code&gt;: Related to the ID&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plant&lt;/code&gt;: Date of planting&lt;/li&gt;
&lt;li&gt;&lt;code&gt;harv&lt;/code&gt;: Date of harvesting&lt;/li&gt;
&lt;li&gt;&lt;code&gt;prev_harv&lt;/code&gt;: Date of previous harves&lt;/li&gt;
&lt;li&gt;&lt;code&gt;field&lt;/code&gt;: Related to the ID&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mill&lt;/code&gt;: which Mill the field belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;variety&lt;/code&gt;: Sugarcane variety planted in the field. Usually also very important for prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;soil-data&#34;&gt;Soil data&lt;/h3&gt;

&lt;p&gt;Some granularity data and chemical properties, I won&amp;rsquo;t get into the specifics like the one before but you can ask in the comments.&lt;/p&gt;

&lt;h3 id=&#34;climate-data&#34;&gt;Climate data&lt;/h3&gt;

&lt;p&gt;we downloaded land surface temperature and accumulated rainfall measures from NASA satellites, and broke them down in different intervals, for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;temp_1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;temp_2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;temp_3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rain_1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rain_2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rain_3&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The number suffix refers to the time interval in relation to the crop planting and harvesting date:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;1&lt;/code&gt; is for 90 days after planting&lt;/li&gt;
&lt;li&gt;&lt;code&gt;3&lt;/code&gt; is for 90 days before harvesting&lt;/li&gt;
&lt;li&gt;&lt;code&gt;2&lt;/code&gt; is for the time between 1 and 3 and may vary depending on management decisions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;modeling-steps&#34;&gt;Modeling steps&lt;/h2&gt;

&lt;p&gt;The usual steps in modeling are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Import the dataset&lt;/li&gt;
&lt;li&gt;Preprocess. In this case, we had to:

&lt;ol&gt;
&lt;li&gt;Deal with missing data&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html&#34; target=&#34;_blank&#34;&gt;Onehot-encode&lt;/a&gt; categorical data&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Cluster data to deal with spatial autocorrelation&lt;/li&gt;
&lt;li&gt;Split between train and test sets&lt;/li&gt;
&lt;li&gt;Optimize hyperparameters of the chosen regression techniques&lt;/li&gt;
&lt;li&gt;Compare the performance of the final models on test data&lt;/li&gt;
&lt;li&gt;Discuss our findings and reason how it compares with a no machine learning approach&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;the-dataset&#34;&gt;The dataset&lt;/h3&gt;

&lt;p&gt;We will start from the fully strucutred dataset, i. e., containing all data described previously. Before getting to this stage we had to do a lot of data collecting and cleaning in order to merge production, soil and climate data into one concise dataset. In total we had about 18000 observations from 4 different mills.&lt;/p&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;field_id&lt;/th&gt;
      &lt;th&gt;farmcod&lt;/th&gt;
      &lt;th&gt;X&lt;/th&gt;
      &lt;th&gt;Y&lt;/th&gt;
      &lt;th&gt;cluster&lt;/th&gt;
      &lt;th&gt;tonnes_harvested&lt;/th&gt;
      &lt;th&gt;prod_env&lt;/th&gt;
      &lt;th&gt;spac&lt;/th&gt;
      &lt;th&gt;harvest_no&lt;/th&gt;
      &lt;th&gt;fire_raw&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;clay&lt;/th&gt;
      &lt;th&gt;frt&lt;/th&gt;
      &lt;th&gt;cec&lt;/th&gt;
      &lt;th&gt;sand&lt;/th&gt;
      &lt;th&gt;sb&lt;/th&gt;
      &lt;th&gt;silt&lt;/th&gt;
      &lt;th&gt;soil&lt;/th&gt;
      &lt;th&gt;txt&lt;/th&gt;
      &lt;th&gt;v&lt;/th&gt;
      &lt;th&gt;test_set&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;12000101001&lt;/td&gt;
      &lt;td&gt;120001&lt;/td&gt;
      &lt;td&gt;387040.077051&lt;/td&gt;
      &lt;td&gt;7.521803e+06&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;55.55&lt;/td&gt;
      &lt;td&gt;G&lt;/td&gt;
      &lt;td&gt;1,4&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;LP&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;12000101002&lt;/td&gt;
      &lt;td&gt;120001&lt;/td&gt;
      &lt;td&gt;387003.564321&lt;/td&gt;
      &lt;td&gt;7.521312e+06&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;48.38&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;1,4&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;LP&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;12000101003&lt;/td&gt;
      &lt;td&gt;120001&lt;/td&gt;
      &lt;td&gt;386857.061114&lt;/td&gt;
      &lt;td&gt;7.521003e+06&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;42.70&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;1,4&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;LP&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;12000101004&lt;/td&gt;
      &lt;td&gt;120001&lt;/td&gt;
      &lt;td&gt;386718.526710&lt;/td&gt;
      &lt;td&gt;7.520828e+06&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;43.41&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;1,4&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;LP&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;12000101005&lt;/td&gt;
      &lt;td&gt;120001&lt;/td&gt;
      &lt;td&gt;386488.755285&lt;/td&gt;
      &lt;td&gt;7.520799e+06&lt;/td&gt;
      &lt;td&gt;95&lt;/td&gt;
      &lt;td&gt;43.84&lt;/td&gt;
      &lt;td&gt;E&lt;/td&gt;
      &lt;td&gt;1,4&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;S&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;LP&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 63 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&#34;number-of-observations-per-mill&#34;&gt;Number of observations per mill&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.mill.value_counts()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;B    5365
D    5051
A    3958
C    3368
Name: mill, dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;clustering-data&#34;&gt;Clustering data&lt;/h2&gt;

&lt;p&gt;We divided each mill in clusters of field due to spatial autocorrelation using a &lt;code&gt;k-means&lt;/code&gt; algorithm.
To choose the right K, we must check, for each mill, how the &lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html&#34; target=&#34;_blank&#34;&gt;inertia&lt;/a&gt; of the dataset varies with the ammount (K) of clusters. Based on that, we choose a low K that leads to a big decrease in inertia without increasing the K value too much. This is called the &lt;a href=&#34;https://pythonprogramminglanguage.com/kmeans-elbow-method/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;elbow method&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
k_list = []
inertia = []
for k in tqdm(np.arange(50,200,10)):
    k_mills_inertia = []
    for mill in df.mill.unique():
        df_to_cluster = df[df.mill == mill]
        coords = np.array(list(zip(df_to_cluster.X,df_to_cluster.Y)))
        kmeans = KMeans(n_clusters=k, random_state=0).fit(coords)
        k_mills_inertia.append(kmeans.inertia_)
    k_list.append(k)
    inertia.append(sum(k_mills_inertia))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;100%|██████████| 15/15 [00:42&amp;lt;00:00,  3.71s/it]

CPU times: user 1min 47s, sys: 4min 6s, total: 5min 53s
Wall time: 42.4 s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Analysing the graph below (this is from one mill, but the others are siminlar), we should look for the elbow, which appears to be between 80 and 110 clusters per mill. I&amp;rsquo;ll use 100 clusters.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.scatter(k_list, inertia)
plt.title(&#39;K for means&#39;)
plt.xlabel(&#39;K&#39;)
plt.ylabel(&#39;Inertia&#39;)
plt.savefig(&#39;./figures/choosing_k.jpg&#39;,dpi = 300)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/yxpPSVrv/choosing-k.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;clustering-observations&#34;&gt;Clustering observations&lt;/h4&gt;

&lt;p&gt;Choosing a &lt;code&gt;K=100&lt;/code&gt; means that for every mill, we will make a 100 clusters of fields based on the &lt;code&gt;X,Y&lt;/code&gt; coordinates of every field. Since the clusters are based only on the coordinates of the centerpoint of every field, this means that nearby fields will be grouped togheter.&lt;/p&gt;

&lt;p&gt;Below, I show how each mill&amp;rsquo;s map is divided after clustering observations and how they are split between &lt;code&gt;training_set&lt;/code&gt;, in blue, and &lt;code&gt;test_set&lt;/code&gt;, in pink.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/QtbShggL/cluster-test-sets-A.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/Vkr7nCwM/cluster-test-sets-B.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/HnkzxJ3r/cluster-test-sets-C.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/0QxZTXwd/cluster-test-sets-D.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;train-and-test-sets&#34;&gt;Train and test sets&lt;/h2&gt;

&lt;p&gt;In order to have a better estimate of how much our model can perform in unseen data we will use the pink fields showed in the graphs above as test fields.
These fields were &lt;strong&gt;manually&lt;/strong&gt; selected and are &lt;strong&gt;at least&lt;/strong&gt; 3km appart from other fields.&lt;/p&gt;

&lt;p&gt;This means that our model will first be trained in the &lt;code&gt;blue&lt;/code&gt; fields and tested in the &lt;code&gt;pink&lt;/code&gt; fields.
During training in the blue fields we will use &lt;a href=&#34;https://scikit-learn.org/stable/modules/cross_validation.html&#34; target=&#34;_blank&#34;&gt;cross-validation&lt;/a&gt; and &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html&#34; target=&#34;_blank&#34;&gt;random search&lt;/a&gt; to optimize each model&amp;rsquo;s hyperparameters.&lt;/p&gt;

&lt;p&gt;After we find the best hyperparameters for every regressor, we train them on the whole blue dataset and compare how they fare on the pink fields, which are further apart and not used at all in training.&lt;/p&gt;

&lt;h2 id=&#34;overview-clustered-vs-naive&#34;&gt;Overview: Clustered vs. Naive&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve split data into &lt;code&gt;mod&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; sets. The &lt;code&gt;test&lt;/code&gt; set is composed by fields which are at least 3km apart from other fields. We will use this data to compare two approaches here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Naive&lt;/strong&gt;: We&amp;rsquo;ll use regular K-fold cross validation in the &lt;code&gt;mod&lt;/code&gt; set to tune hyperparameters and get the best cross validaton error results. As we&amp;rsquo;ll see below, this approach can send nearby, and, therefore, similar, if spatial autocorrelation is present), to &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; sets. This way, the model is trained and tested in more correlated observations.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Grouped&lt;/strong&gt;: We&amp;rsquo;ll usa a group K-fold cross validation grouping observations by clusters to tune hyperparameters on the &lt;code&gt;mod&lt;/code&gt; set. This approach makes it less likely that nearby fields be sent to different folds suring cross validation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the graphs below we can se how a mill&amp;rsquo;s data was split during two different steps of the 5 fold cross validations. CV test sets are in green and CV training set are in gray. The main takeaway here is to note how nearby observations are kept in different sets, except for border cases, in the &lt;code&gt;Clustered&lt;/code&gt; approach. In the &lt;code&gt;Naive&lt;/code&gt; one, test fields are scattered throughout the dataset and nearby fields can go to training and test sets during CV, which, as we&amp;rsquo;ll se, yields an optimistic Cross Validation Mean Absolute Error.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./figures/cv_compare_split_0_mill_D.jpghttps://i.postimg.cc/yN0vrCk8/cv-compare-split-0-mill-D.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/VkxKRksh/cv-compare-split-4-mill-D.jpg./figures/cv_compare_split_4_mill_D.jpg&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;hyperparameter-optimization&#34;&gt;Hyperparameter optimization&lt;/h2&gt;

&lt;p&gt;As for the modeling techniques, we compared how &lt;code&gt;XGBoost&lt;/code&gt; (from now on &lt;code&gt;xgb&lt;/code&gt;), &lt;code&gt;Random Forest&lt;/code&gt; (&lt;code&gt;rf&lt;/code&gt;) and &lt;code&gt;Support Vector Regression&lt;/code&gt; (&lt;code&gt;svr&lt;/code&gt;), perform on this kind of dataset. The hyperparameters tuned are specified below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xgb_hyperparameters = {&#39;eta&#39;: np.array([0.08, 0.9 , 0.28]),
                       &#39;max_depth&#39;: [1, 3, 5, 7],
                       &#39;min_child_weight&#39;: np.array([ 1,  3,  5,  7,  9, 11])}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;svr_hyperparameters = {
    &#39;C&#39; : 10.**np.arange(-5,3,1),
    &#39;epsilon&#39; : 10.**np.arange(-8,0,1)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rf_hyperparameters = {
    &#39;n_estimators&#39;: np.arange(100,500,100),
    &#39;max_depth&#39;: np.arange(1,9,2),
    &#39;max_features&#39;:[3,5,7,11,15,20]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I will leave the sample code below only because it summarises the steps taken in a concise way. I know it can be improved, but I&amp;rsquo;ll leave that for the kernel I plan to write as this is not the focus of the post.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
all_mills = {}
for mill in [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;]:
    &amp;quot;&amp;quot;&amp;quot;
    To predict for every mill, we only have to filter mod by mill here
    &amp;quot;&amp;quot;&amp;quot;
    X_mod = mod.loc[(mod.test_set == 0)&amp;amp;(mod.mill == mill),np.hstack([categoric_columns,numeric_columns])]
    y_mod = mod[&#39;tonnes_harvested&#39;].loc[(mod.test_set == 0)&amp;amp;(mod.mill == mill)]
    X_test = mod.loc[(mod.test_set == 1)&amp;amp;(mod.mill == mill),np.hstack([categoric_columns,numeric_columns])]
    y_test = mod[&#39;tonnes_harvested&#39;].loc[(mod.test_set == 1)&amp;amp;(mod.mill == mill)]
    #preprocess pipeline
    X_mod = preprocess.fit_transform(X_mod)
    X_test = preprocess.fit_transform(X_test)
    X_test = X_test.reindex(columns = X_mod.columns, 
                                  fill_value=0)


    #generate NAIVE cv splits
    naive_cv = KFold(n_splits = 5,shuffle=True)
    naive_cv.get_n_splits(X_mod,y_mod)
    naive_splits = list(naive_cv.split(X_mod,y_mod))

    #generate GROUPS cv splits
    groups_mod = mod[&#39;cluster_id&#39;].loc[(mod.test_set == 0)&amp;amp;(mod.mill == mill)]
    grouped_cv = GroupKFold(n_splits=5)
    grouped_cv.get_n_splits(X_mod,y_mod,groups_mod)
    grouped_splits = list(grouped_cv.split(X_mod,y_mod,groups_mod))


    #test all estimators
    grouped_result_dict = {}
    naive_result_dict = {}
    estimators = [XGBRegressor(),RandomForestRegressor(),SVR()]
    hyperparameters = [xgb_hyperparameters,rf_hyperparameters,svr_hyperparameters]
    model_ids = [&#39;xgb&#39;,&#39;rf&#39;,&#39;svr&#39;]

    for est,hyper,ids in zip(estimators,hyperparameters,model_ids):
        grouped_result_dict[ids] = Results(est,hyper, n_iter=number_iterations,
                                    cv_split = grouped_splits,
                                    scoring=&#39;neg_mean_absolute_error&#39;,
                                    refit = True,
                                    X = X_mod,y = y_mod, X_test=X_test,y_test=y_test,mod_id=ids)
        grouped_result_dict[ids].get_results()

    for est,hyper,ids in zip(estimators,hyperparameters,model_ids):
        naive_result_dict[ids] = Results(est,hyper, n_iter=number_iterations,
                                    cv_split = naive_splits,
                                    scoring=&#39;neg_mean_absolute_error&#39;,
                                    refit = True,
                                    X = X_mod,y = y_mod, X_test=X_test,y_test=y_test,mod_id=ids)
        naive_result_dict[ids].get_results()

    #Generate a dataframe with all results
    final_results =  pd.DataFrame({&#39;Estimator&#39; : [&#39;XGB&#39;,&#39;Random Forest&#39;,&#39;SVR&#39;],
                                   &#39;Grouped CV MAE&#39; : [abs(x.estimator.best_score_) for x in grouped_result_dict.values()],
                                   &#39;Naive CV MAE&#39; : [abs(x.estimator.best_score_) for x in naive_result_dict.values()],
                                   &#39;Grouped Test MAE&#39; : [x.test_mae for x in grouped_result_dict.values()],
                                   &#39;Naive Test MAE&#39; : [x.test_mae for x in naive_result_dict.values()]})
    final_results[&#39;Grouped CV - Test Error&#39;] = abs(final_results[&#39;Grouped CV MAE&#39;] - final_results[&#39;Grouped Test MAE&#39;])
    final_results[&#39;Naive CV - Test Error&#39;] = abs(final_results[&#39;Naive CV MAE&#39;] - final_results[&#39;Naive Test MAE&#39;])
    final_results[&#39;Mill&#39;] = mill
    all_mills[mill] = final_results
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;The table below summarises the results found for every mill and technique. We compared the cross validation error for the &lt;code&gt;grouped&lt;/code&gt; and &lt;code&gt;naive&lt;/code&gt; aproaches and how it differes from the test error in both scenarios. All numbers are &lt;code&gt;tonnes per hectare&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_results
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;estimator&lt;/th&gt;
      &lt;th&gt;grouped_cv_mae&lt;/th&gt;
      &lt;th&gt;naive_cv_mae&lt;/th&gt;
      &lt;th&gt;grouped_test_mae&lt;/th&gt;
      &lt;th&gt;naive_test_mae&lt;/th&gt;
      &lt;th&gt;grouped_cv_test_error&lt;/th&gt;
      &lt;th&gt;naive_cv_test_error&lt;/th&gt;
      &lt;th&gt;mill&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;XGB&lt;/td&gt;
      &lt;td&gt;12.298977&lt;/td&gt;
      &lt;td&gt;8.245792&lt;/td&gt;
      &lt;td&gt;13.997387&lt;/td&gt;
      &lt;td&gt;13.997387&lt;/td&gt;
      &lt;td&gt;1.698411&lt;/td&gt;
      &lt;td&gt;5.751595&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;13.625747&lt;/td&gt;
      &lt;td&gt;12.048800&lt;/td&gt;
      &lt;td&gt;14.914672&lt;/td&gt;
      &lt;td&gt;14.929350&lt;/td&gt;
      &lt;td&gt;1.288925&lt;/td&gt;
      &lt;td&gt;2.880551&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;SVR&lt;/td&gt;
      &lt;td&gt;13.514483&lt;/td&gt;
      &lt;td&gt;6.850801&lt;/td&gt;
      &lt;td&gt;15.184413&lt;/td&gt;
      &lt;td&gt;15.184414&lt;/td&gt;
      &lt;td&gt;1.669931&lt;/td&gt;
      &lt;td&gt;8.333613&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;XGB&lt;/td&gt;
      &lt;td&gt;11.837691&lt;/td&gt;
      &lt;td&gt;6.785474&lt;/td&gt;
      &lt;td&gt;11.895676&lt;/td&gt;
      &lt;td&gt;11.882763&lt;/td&gt;
      &lt;td&gt;0.057985&lt;/td&gt;
      &lt;td&gt;5.097290&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;12.675220&lt;/td&gt;
      &lt;td&gt;10.280449&lt;/td&gt;
      &lt;td&gt;12.531138&lt;/td&gt;
      &lt;td&gt;12.635846&lt;/td&gt;
      &lt;td&gt;0.144082&lt;/td&gt;
      &lt;td&gt;2.355396&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;SVR&lt;/td&gt;
      &lt;td&gt;12.431233&lt;/td&gt;
      &lt;td&gt;6.058838&lt;/td&gt;
      &lt;td&gt;13.856015&lt;/td&gt;
      &lt;td&gt;13.854876&lt;/td&gt;
      &lt;td&gt;1.424783&lt;/td&gt;
      &lt;td&gt;7.796038&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;XGB&lt;/td&gt;
      &lt;td&gt;13.383667&lt;/td&gt;
      &lt;td&gt;7.591855&lt;/td&gt;
      &lt;td&gt;17.288527&lt;/td&gt;
      &lt;td&gt;17.490687&lt;/td&gt;
      &lt;td&gt;3.904859&lt;/td&gt;
      &lt;td&gt;9.898832&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;14.191126&lt;/td&gt;
      &lt;td&gt;10.698596&lt;/td&gt;
      &lt;td&gt;16.503020&lt;/td&gt;
      &lt;td&gt;16.498087&lt;/td&gt;
      &lt;td&gt;2.311894&lt;/td&gt;
      &lt;td&gt;5.799491&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;SVR&lt;/td&gt;
      &lt;td&gt;16.039336&lt;/td&gt;
      &lt;td&gt;7.905194&lt;/td&gt;
      &lt;td&gt;18.839928&lt;/td&gt;
      &lt;td&gt;18.839924&lt;/td&gt;
      &lt;td&gt;2.800592&lt;/td&gt;
      &lt;td&gt;10.934730&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;XGB&lt;/td&gt;
      &lt;td&gt;10.355872&lt;/td&gt;
      &lt;td&gt;5.868488&lt;/td&gt;
      &lt;td&gt;13.835605&lt;/td&gt;
      &lt;td&gt;13.835605&lt;/td&gt;
      &lt;td&gt;3.479733&lt;/td&gt;
      &lt;td&gt;7.967117&lt;/td&gt;
      &lt;td&gt;D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Random Forest&lt;/td&gt;
      &lt;td&gt;12.142127&lt;/td&gt;
      &lt;td&gt;10.096490&lt;/td&gt;
      &lt;td&gt;15.038501&lt;/td&gt;
      &lt;td&gt;15.265431&lt;/td&gt;
      &lt;td&gt;2.896373&lt;/td&gt;
      &lt;td&gt;5.168941&lt;/td&gt;
      &lt;td&gt;D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;SVR&lt;/td&gt;
      &lt;td&gt;11.182533&lt;/td&gt;
      &lt;td&gt;4.675396&lt;/td&gt;
      &lt;td&gt;14.465358&lt;/td&gt;
      &lt;td&gt;14.465348&lt;/td&gt;
      &lt;td&gt;3.282825&lt;/td&gt;
      &lt;td&gt;9.789953&lt;/td&gt;
      &lt;td&gt;D&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We can first say that even though some models have the same performance on the test sets for either the &lt;code&gt;grouped&lt;/code&gt; or &lt;code&gt;naive&lt;/code&gt; approach, the difference between &lt;code&gt;cross valitaion error&lt;/code&gt; is much smaller for the &lt;code&gt;grouped&lt;/code&gt; approach. This means that we can better estimate how a model will perform with unseen data by using the &lt;code&gt;grouped&lt;/code&gt; approach and cluster data before training.&lt;/p&gt;

&lt;p&gt;For example, using the results for mill &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;xgb&lt;/code&gt; (1st line). The &lt;code&gt;grouped&lt;/code&gt; CV mean absolute error was 12.3, the naive was &lt;code&gt;8.2&lt;/code&gt;. The actual error in the test set was the same for both but using a &lt;code&gt;grouped&lt;/code&gt; cross validation leads to a much smaller difference between CVverror than using a &lt;code&gt;naive&lt;/code&gt; approach (1.7 for &lt;code&gt;grouped&lt;/code&gt; vs 5.8 for &lt;code&gt;naive&lt;/code&gt;). The graph bellow gives us a better view of this pattern.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/TYH7Kczf/diff-cv-test-mae.jpgoutput_62_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;If we were to predict each field&amp;rsquo;s yield using the average tonnes per hectare harvested, the &lt;code&gt;mean absolute error&lt;/code&gt; would be vary between 17 to 18 tonnes per hectare, depending on the mill. By using these models, we might reduce (using &lt;code&gt;xgb&lt;/code&gt; results as an example) these to 13-14 tonnes per hectare. That is a 18% decrease in error prediction. If a common sugarcane carrier truck can transport 15tonnes, this may be significant to take into consideration when planning harvest season.&lt;/p&gt;

&lt;p&gt;Even though it does not seem like a big improvement and besides using the mean we can also segment the mean by number of harvests and variety, etc. this dataset was acquired between 2010 and 2014. Since we&amp;rsquo;re in 2019 and data adequate data acquisition is more prioritized, modern datasets may &lt;em&gt;yield&lt;/em&gt; better results. Another important point to note is that this approach is independent of the selected crop. We can use the same routine to predict soy, corn, cotton and many other main crop, given an adequately sized dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FEG18 Aula05</title>
      <link>https://mferrac.github.io/post/feg18-aula05/</link>
      <pubDate>Mon, 27 May 2019 11:21:54 -0300</pubDate>
      
      <guid>https://mferrac.github.io/post/feg18-aula05/</guid>
      <description>

&lt;h2 id=&#34;aula-05&#34;&gt;Aula 05&lt;/h2&gt;

&lt;p&gt;Na aula de hoje, usaremos os dados e códigos da &lt;a href=&#34;https://drive.google.com/drive/u/0/folders/1yn_5FuPA_wRddyM7WR8_fo94-6DDut3W&#34; target=&#34;_blank&#34;&gt;pasta da Aula 05&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;1ª-parte&#34;&gt;1ª parte&lt;/h3&gt;

&lt;p&gt;Na primeira parte da aula trabalharemos com dados sintéticos, denominados &lt;code&gt;first&lt;/code&gt;, &lt;code&gt;second&lt;/code&gt; e &lt;code&gt;third&lt;/code&gt; &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;2ª-parte&#34;&gt;2ª parte&lt;/h3&gt;

&lt;p&gt;Na segunda parte, usaremos o conjunto de dados de cereais (&lt;code&gt;cereals.csv&lt;/code&gt;).
Com os atributos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt; : Name of cereal&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mfr&lt;/strong&gt; : Manufacturer of cereal

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt; = American Home Food Products;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G&lt;/strong&gt; = General Mills&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K&lt;/strong&gt; = Kelloggs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;N&lt;/strong&gt; = Nabisco&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;P&lt;/strong&gt; = Post&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt; = Quaker Oats&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R&lt;/strong&gt; = Ralston Purina&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;type&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;cold&lt;/li&gt;
&lt;li&gt;hot&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;calories&lt;/strong&gt;: calories per serving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;protein&lt;/strong&gt;: grams of protein&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fat&lt;/strong&gt;: grams of fat&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sodium&lt;/strong&gt;: milligrams of sodium&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fiber&lt;/strong&gt;: grams of dietary fiber&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;carbo&lt;/strong&gt;: grams of complex carbohydrates&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sugars&lt;/strong&gt;: grams of sugars&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;potass&lt;/strong&gt;: milligrams of potassium&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;vitamins&lt;/strong&gt;: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;shelf&lt;/strong&gt;: display shelf (1, 2, or 3, counting from the floor)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;weight&lt;/strong&gt;: weight in ounces of one serving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cups&lt;/strong&gt;: number of cups in one serving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rating&lt;/strong&gt;: a rating of the cereals (Possibly from Consumer Reports?)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Análise de Dados - Aula 04</title>
      <link>https://mferrac.github.io/post/feg18-aula04/</link>
      <pubDate>Mon, 20 May 2019 12:00:50 -0300</pubDate>
      
      <guid>https://mferrac.github.io/post/feg18-aula04/</guid>
      <description>

&lt;h1 id=&#34;dados-da-aula&#34;&gt;Dados da aula&lt;/h1&gt;

&lt;p&gt;Os dados podem ser baixados &lt;a href=&#34;https://drive.google.com/drive/folders/1yn_5FuPA_wRddyM7WR8_fo94-6DDut3W?usp=sharing&#34; target=&#34;_blank&#34;&gt;na pasta da Aula 04&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;instalações&#34;&gt;Instalações&lt;/h1&gt;

&lt;p&gt;Para esta aula, serão necessários os pacotes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: Implementação de &lt;code&gt;knn&lt;/code&gt; para classificação&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FNN&lt;/code&gt;: Tem uma implementação de &lt;code&gt;knn&lt;/code&gt; para regressão&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Análise de Dados - Aula 03</title>
      <link>https://mferrac.github.io/post/feg18-aula03/</link>
      <pubDate>Sun, 12 May 2019 16:13:50 -0300</pubDate>
      
      <guid>https://mferrac.github.io/post/feg18-aula03/</guid>
      <description>

&lt;h2 id=&#34;homework&#34;&gt;Homework&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Baixe os dados na &lt;a href=&#34;https://drive.google.com/drive/folders/1yn_5FuPA_wRddyM7WR8_fo94-6DDut3W?usp=sharing&#34; target=&#34;_blank&#34;&gt;pasta Homework da Aula 03&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Importe os dados no Rstudio&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Envie as respostas pelo &lt;a href=&#34;https://forms.gle/3EmTaTxzfYEx43oG9&#34; target=&#34;_blank&#34;&gt;formulário&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;dados-para-a-aula&#34;&gt;Dados para a aula&lt;/h1&gt;

&lt;p&gt;Baixar em &lt;a href=&#34;https://transfer.sh/I109G/aula03.zip&#34; target=&#34;_blank&#34;&gt;https://transfer.sh/I109G/aula03.zip&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;instalações&#34;&gt;Instalações&lt;/h1&gt;

&lt;p&gt;Para esta aula, serão necessários os pacotes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mlr&lt;/code&gt;: Esta bibloteca serve com interface para os algoritmos de machine learning usados no curso.
Mais informações no &lt;a href=&#34;https://mlr.mlr-org.com/&#34; target=&#34;_blank&#34;&gt;site&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rpart&lt;/code&gt;: A implementação do algoritmo de árvores de decisão. Mais informações no &lt;a href=&#34;https://cran.r-project.org/web/packages/rpart/rpart.pdf&#34; target=&#34;_blank&#34;&gt;manual&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rpart.plot&lt;/code&gt;: Contém funções para visualização da árvore de decisão.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tidyverse&lt;/code&gt;: Instalado na aula 02.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para instalar cada um destes pacotes, basta usar o comandos &lt;code&gt;install.packages(&amp;quot;nome_do_pacote&amp;quot;)&lt;/code&gt;. Por exemplo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&amp;quot;mlr&amp;quot;)
install.packages(&amp;quot;rpart&amp;quot;)
install.packages(&amp;quot;rpart.plot&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para instalar todos os pacotes de uma vez, use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(c(&amp;quot;mlr&amp;quot;,&amp;quot;rpart&amp;quot;,&amp;quot;rpart.plot&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Análise de Dados - Aula 02</title>
      <link>https://mferrac.github.io/post/feg18-aula02/</link>
      <pubDate>Wed, 01 May 2019 09:32:50 -0300</pubDate>
      
      <guid>https://mferrac.github.io/post/feg18-aula02/</guid>
      <description>

&lt;h2 id=&#34;homework&#34;&gt;Homework&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Baixe os dados em &lt;a href=&#34;https://transfer.sh/ElXkS/adult_data.csv&#34; target=&#34;_blank&#34;&gt;https://transfer.sh/ElXkS/adult_data.csv&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Importe os dados no Rstudio&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Envie as respostas pelo &lt;a href=&#34;https://forms.gle/UQxbW6csWnfFWMw69&#34; target=&#34;_blank&#34;&gt;formulário&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;arquivos-da-aula&#34;&gt;Arquivos da aula&lt;/h2&gt;

&lt;p&gt;O notebook da aula pode ser baixado &lt;a href=&#34;https://kyso.io/mferrac/analise-de-dados-02-manipulacao-de-dados&#34; target=&#34;_blank&#34;&gt;neste link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Os arquivos da aula podem ser baixados &lt;a href=&#34;http://bit.ly/feagri_analise_dados_aula02&#34; target=&#34;_blank&#34;&gt;neste link&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;instalação-do-jupyter-com-r&#34;&gt;Instalação do Jupyter com R&lt;/h2&gt;

&lt;p&gt;Para instalar o Jupyter notebook no windows, primeiro é preciso instalar o anaconda no link:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/distribution/&#34; target=&#34;_blank&#34;&gt;https://www.anaconda.com/distribution/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Selecione &amp;ldquo;windows&amp;rdquo; e clique em &lt;code&gt;Download&lt;/code&gt; abaixo de &lt;code&gt;Python 3.7 Version&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Em seguida, execute o arquivo &lt;code&gt;Anaconda3-2019.03-Windows-x86_64.exe&lt;/code&gt; e siga as instruções de instalação.&lt;/p&gt;

&lt;p&gt;Após instalar, procure &lt;code&gt;Anaconda Prompt&lt;/code&gt; na pesquisa do Windows e execute o programa.&lt;/p&gt;

&lt;p&gt;No terminal aberto, digite o comando:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda install -c r r-irkernel
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;básicos-de-r-com-swirl&#34;&gt;Básicos de R com &lt;code&gt;swirl&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Para instalar o pacote &lt;code&gt;swirl&lt;/code&gt;, basta executar o comando abaixo. Este comando instala o pacote na sua máquina. As aspas são obrigatórias. Uma vez instalado, não precisa executar de novo.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#39;swirl&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para carregar o pacote, use &lt;code&gt;library(swirl)&lt;/code&gt; ou &lt;code&gt;library(&#39;swirl&#39;)&lt;/code&gt;. Este comando ativa o pacote na sessão aberta do R e deve ser executado sempre que se reinicia o programa. As aspas são opcionais.&lt;/p&gt;

&lt;p&gt;Para começar o tutorial, após executar o comando acima, digite:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;swirl()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Em seguida, siga o tutorial que vai começar na tela. Coloque um nome para continuar de onde parou e manter o progresso de cada sessão. Recomendamos fazer as sessões de &lt;strong&gt;1 a 4&lt;/strong&gt;, no mínimo para a aula 02. Idealmente, façam quantas puderem, até a 8.&lt;/p&gt;

&lt;p&gt;É melhor fazer uma ou duas sessões por dia, do que todas de uma vez no final de semana.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lembrem-se que para o &lt;code&gt;swirl&lt;/code&gt; o correto é usar &lt;code&gt;&amp;lt;-&lt;/code&gt; no lugar de &lt;code&gt;=&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

[{"authors":["admin"],"categories":null,"content":"An Agricultural Engineer turned data scientist by crop yield modeling. I started using machine learning to study the effects of spatial autocorrelation in empirical modeling of sugarcane yield, coffee disease occurrence prediction and soil classification models. Outside of agriculture, I have developed projects on fraud detection, lawsuit outcome prediction, customer churning, logistics transportation, car GPS tracking, students performance on online education and survey data analysis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://mferrac.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"An Agricultural Engineer turned data scientist by crop yield modeling. I started using machine learning to study the effects of spatial autocorrelation in empirical modeling of sugarcane yield, coffee disease occurrence prediction and soil classification models. Outside of agriculture, I have developed projects on fraud detection, lawsuit outcome prediction, customer churning, logistics transportation, car GPS tracking, students performance on online education and survey data analysis.","tags":null,"title":"Matheus Agostini Ferraciolli","type":"author"},{"authors":null,"categories":null,"content":" Yield modeling The harvesting operation involves transportation of heavy agricultural machinery across great distances in the fields. Effective management of field operations can improve adequate machine use planning, which, in turn can reduce costs related to fuel and workforce allocation.\nIf we can more accurately predict how much will be harvested in each field, we can have a better planning of the whole operation. In this post I\u0026rsquo;ll discuss how we can use available data and moder machine learning algorthims to improve harvest prediction.\nPlease note that the data is not allowed to be shared, so I\u0026rsquo;ll just post this as a guideline of what we can usually consider when modelling yield and show some of the insights we had along the way.\nMy first publication regarded yield modeling for sugarcane comparing We used a dataset containing, for every field:\n Production data for 4 diferent mills across 4 years of harvest Climate data acquired from satelites and crossed on field level using shapefiles of every field Soil data  In the end, we developed models to predict, on field level, how much will be harvested in that year. Below, I\u0026rsquo;ll explain the steps taken to model a problem like this. The same procedure may be applied to many other crops, given an available dataset.\ndf = pd.read_csv('/home/mferrac/data/sugarcane.csv') df.head()  Each row represents data from a whole cicle. Each column (or attribute) represents data from a whole cicle. The problem we aim to solve is \u0026ldquo;given a field had a cycle with given attributes, how much will be harvested there?\u0026rdquo;. There is the bigger problem where we might not have some of these atributes like temperature, rainfall, etc. in advance, but I\u0026rsquo;ll leave that for another post.\nAvailable attributes In total, there were 63 attributes available, divided in production, soil, climate and chemical input data.\nProduction data  field_id : ID\u0026rsquo;s not used for prediction farmcod: Part of the ID test: 0 or 1 if it is in the test_set or not X: Longitude Y: Latitude tonnes_harvested: Tonnes harvested in the field (what we aim to predict) prod_env: Grade (AB, C, D, E, etc.) related to the local conditions to produce sugarcane spac: Spacing betweeen rows harvest_no: Number of harvests. As a semiperennial culture, sugarcane is harvested more than once. It is usually an important attribute. fire_raw: Was fire used or not before harvesting no_field: Related to the ID plant: Date of planting harv: Date of harvesting prev_harv: Date of previous harves field: Related to the ID mill: which Mill the field belongs to variety: Sugarcane variety planted in the field. Usually also very important for prediction.  Soil data Some granularity data and chemical properties, I won\u0026rsquo;t get into the specifics like the one before but you can ask in the comments.\nClimate data we downloaded land surface temperature and accumulated rainfall measures from NASA satellites, and broke them down in different intervals, for example:\n temp_1 temp_2 temp_3 rain_1 rain_2 rain_3  The number suffix refers to the time interval in relation to the crop planting and harvesting date:\n 1 is for 90 days after planting 3 is for 90 days before harvesting 2 is for the time between 1 and 3 and may vary depending on management decisions  Modeling steps The usual steps in modeling are:\n Import the dataset Preprocess. In this case, we had to:  Deal with missing data Onehot-encode categorical data  Cluster data to deal with spatial autocorrelation Split between train and test sets Optimize hyperparameters of the chosen regression techniques Compare the performance of the final models on test data Discuss our findings and reason how it compares with a no machine learning approach  The dataset We will start from the fully strucutred dataset, i. e., containing all data described previously. Before getting to this stage we had to do a lot of data collecting and cleaning in order to merge production, soil and climate data into one concise dataset. In total we had about 18000 observations from 4 different mills.\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    field_id farmcod X Y cluster tonnes_harvested prod_env spac harvest_no fire_raw ... clay frt cec sand sb silt soil txt v test_set     0 12000101001 120001 387040.077051 7.521803e+06 95 55.55 G 1,4 6 S ... NaN 3.0 NaN NaN NaN NaN LP 4.0 NaN False   1 12000101002 120001 387003.564321 7.521312e+06 95 48.38 E 1,4 6 S ... NaN 3.0 NaN NaN NaN NaN LP 4.0 NaN False   2 12000101003 120001 386857.061114 7.521003e+06 95 42.70 E 1,4 6 S ... NaN 3.0 NaN NaN NaN NaN LP 4.0 NaN False   3 12000101004 120001 386718.526710 7.520828e+06 95 43.41 E 1,4 6 S ... NaN 3.0 NaN NaN NaN NaN LP 4.0 NaN False   4 12000101005 120001 386488.755285 7.520799e+06 95 43.84 E 1,4 6 S ... NaN 3.0 NaN NaN NaN NaN LP 4.0 NaN False    5 rows × 63 columns\n Number of observations per mill df.mill.value_counts()  B 5365 D 5051 A 3958 C 3368 Name: mill, dtype: int64  Clustering data We divided each mill in clusters of field due to spatial autocorrelation using a k-means algorithm. To choose the right K, we must check, for each mill, how the inertia of the dataset varies with the ammount (K) of clusters. Based on that, we choose a low K that leads to a big decrease in inertia without increasing the K value too much. This is called the \u0026ldquo;elbow method\u0026rdquo;.\n%%time k_list = [] inertia = [] for k in tqdm(np.arange(50,200,10)): k_mills_inertia = [] for mill in df.mill.unique(): df_to_cluster = df[df.mill == mill] coords = np.array(list(zip(df_to_cluster.X,df_to_cluster.Y))) kmeans = KMeans(n_clusters=k, random_state=0).fit(coords) k_mills_inertia.append(kmeans.inertia_) k_list.append(k) inertia.append(sum(k_mills_inertia))  100%|██████████| 15/15 [00:42\u0026lt;00:00, 3.71s/it] CPU times: user 1min 47s, sys: 4min 6s, total: 5min 53s Wall time: 42.4 s  Analysing the graph below (this is from one mill, but the others are siminlar), we should look for the elbow, which appears to be between 80 and 110 clusters per mill. I\u0026rsquo;ll use 100 clusters.\nplt.scatter(k_list, inertia) plt.title('K for means') plt.xlabel('K') plt.ylabel('Inertia') plt.savefig('./figures/choosing_k.jpg',dpi = 300)  Clustering observations Choosing a K=100 means that for every mill, we will make a 100 clusters of fields based on the X,Y coordinates of every field. Since the clusters are based only on the coordinates of the centerpoint of every field, this means that nearby fields will be grouped togheter.\nBelow, I show how each mill\u0026rsquo;s map is divided after clustering observations and how they are split between training_set, in blue, and test_set, in pink.\nTrain and test sets In order to have a better estimate of how much our model can perform in unseen data we will use the pink fields showed in the graphs above as test fields. These fields were manually selected and are at least 3km appart from other fields.\nThis means that our model will first be trained in the blue fields and tested in the pink fields. During training in the blue fields we will use cross-validation and random search to optimize each model\u0026rsquo;s hyperparameters.\nAfter we find the best hyperparameters for every regressor, we train them on the whole blue dataset and compare how they fare on the pink fields, which are further apart and not used at all in training.\nOverview: Clustered vs. Naive We\u0026rsquo;ve split data into mod and test sets. The test set is composed by fields which are at least 3km apart from other fields. We will use this data to compare two approaches here:\n Naive: We\u0026rsquo;ll use regular K-fold cross validation in the mod set to tune hyperparameters and get the best cross validaton error results. As we\u0026rsquo;ll see below, this approach can send nearby, and, therefore, similar, if spatial autocorrelation is present), to train and test sets. This way, the model is trained and tested in more correlated observations.\n Grouped: We\u0026rsquo;ll usa a group K-fold cross validation grouping observations by clusters to tune hyperparameters on the mod set. This approach makes it less likely that nearby fields be sent to different folds suring cross validation.\n  In the graphs below we can se how a mill\u0026rsquo;s data was split during two different steps of the 5 fold cross validations. CV test sets are in green and CV training set are in gray. The main takeaway here is to note how nearby observations are kept in different sets, except for border cases, in the Clustered approach. In the Naive one, test fields are scattered throughout the dataset and nearby fields can go to training and test sets during CV, which, as we\u0026rsquo;ll se, yields an optimistic Cross Validation Mean Absolute Error.\nHyperparameter optimization As for the modeling techniques, we compared how XGBoost (from now on xgb), Random Forest (rf) and Support Vector Regression (svr), perform on this kind of dataset. The hyperparameters tuned are specified below.\nxgb_hyperparameters = {'eta': np.array([0.08, 0.9 , 0.28]), 'max_depth': [1, 3, 5, 7], 'min_child_weight': np.array([ 1, 3, 5, 7, 9, 11])}  svr_hyperparameters = { 'C' : 10.**np.arange(-5,3,1), 'epsilon' : 10.**np.arange(-8,0,1) }  rf_hyperparameters = { 'n_estimators': np.arange(100,500,100), 'max_depth': np.arange(1,9,2), 'max_features':[3,5,7,11,15,20] }  I will leave the sample code below only because it summarises the steps taken in a concise way. I know it can be improved, but I\u0026rsquo;ll leave that for the kernel I plan to write as this is not the focus of the post.\n%%time all_mills = {} for mill in ['A','B','C','D']: \u0026quot;\u0026quot;\u0026quot; To predict for every mill, we only have to filter mod by mill here \u0026quot;\u0026quot;\u0026quot; X_mod = mod.loc[(mod.test_set == 0)\u0026amp;(mod.mill == mill),np.hstack([categoric_columns,numeric_columns])] y_mod = mod['tonnes_harvested'].loc[(mod.test_set == 0)\u0026amp;(mod.mill == mill)] X_test = mod.loc[(mod.test_set == 1)\u0026amp;(mod.mill == mill),np.hstack([categoric_columns,numeric_columns])] y_test = mod['tonnes_harvested'].loc[(mod.test_set == 1)\u0026amp;(mod.mill == mill)] #preprocess pipeline X_mod = preprocess.fit_transform(X_mod) X_test = preprocess.fit_transform(X_test) X_test = X_test.reindex(columns = X_mod.columns, fill_value=0) #generate NAIVE cv splits naive_cv = KFold(n_splits = 5,shuffle=True) naive_cv.get_n_splits(X_mod,y_mod) naive_splits = list(naive_cv.split(X_mod,y_mod)) #generate GROUPS cv splits groups_mod = mod['cluster_id'].loc[(mod.test_set == 0)\u0026amp;(mod.mill == mill)] grouped_cv = GroupKFold(n_splits=5) grouped_cv.get_n_splits(X_mod,y_mod,groups_mod) grouped_splits = list(grouped_cv.split(X_mod,y_mod,groups_mod)) #test all estimators grouped_result_dict = {} naive_result_dict = {} estimators = [XGBRegressor(),RandomForestRegressor(),SVR()] hyperparameters = [xgb_hyperparameters,rf_hyperparameters,svr_hyperparameters] model_ids = ['xgb','rf','svr'] for est,hyper,ids in zip(estimators,hyperparameters,model_ids): grouped_result_dict[ids] = Results(est,hyper, n_iter=number_iterations, cv_split = grouped_splits, scoring='neg_mean_absolute_error', refit = True, X = X_mod,y = y_mod, X_test=X_test,y_test=y_test,mod_id=ids) grouped_result_dict[ids].get_results() for est,hyper,ids in zip(estimators,hyperparameters,model_ids): naive_result_dict[ids] = Results(est,hyper, n_iter=number_iterations, cv_split = naive_splits, scoring='neg_mean_absolute_error', refit = True, X = X_mod,y = y_mod, X_test=X_test,y_test=y_test,mod_id=ids) naive_result_dict[ids].get_results() #Generate a dataframe with all results final_results = pd.DataFrame({'Estimator' : ['XGB','Random Forest','SVR'], 'Grouped CV MAE' : [abs(x.estimator.best_score_) for x in grouped_result_dict.values()], 'Naive CV MAE' : [abs(x.estimator.best_score_) for x in naive_result_dict.values()], 'Grouped Test MAE' : [x.test_mae for x in grouped_result_dict.values()], 'Naive Test MAE' : [x.test_mae for x in naive_result_dict.values()]}) final_results['Grouped CV - Test Error'] = abs(final_results['Grouped CV MAE'] - final_results['Grouped Test MAE']) final_results['Naive CV - Test Error'] = abs(final_results['Naive CV MAE'] - final_results['Naive Test MAE']) final_results['Mill'] = mill all_mills[mill] = final_results  Discussion The table below summarises the results found for every mill and technique. We compared the cross validation error for the grouped and naive aproaches and how it differes from the test error in both scenarios. All numbers are tonnes per hectare.\nmodel_results   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    estimator grouped_cv_mae naive_cv_mae grouped_test_mae naive_test_mae grouped_cv_test_error naive_cv_test_error mill     0 XGB 12.298977 8.245792 13.997387 13.997387 1.698411 5.751595 A   1 Random Forest 13.625747 12.048800 14.914672 14.929350 1.288925 2.880551 A   2 SVR 13.514483 6.850801 15.184413 15.184414 1.669931 8.333613 A   0 XGB 11.837691 6.785474 11.895676 11.882763 0.057985 5.097290 B   1 Random Forest 12.675220 10.280449 12.531138 12.635846 0.144082 2.355396 B   2 SVR 12.431233 6.058838 13.856015 13.854876 1.424783 7.796038 B   0 XGB 13.383667 7.591855 17.288527 17.490687 3.904859 9.898832 C   1 Random Forest 14.191126 10.698596 16.503020 16.498087 2.311894 5.799491 C   2 SVR 16.039336 7.905194 18.839928 18.839924 2.800592 10.934730 C   0 XGB 10.355872 5.868488 13.835605 13.835605 3.479733 7.967117 D   1 Random Forest 12.142127 10.096490 15.038501 15.265431 2.896373 5.168941 D   2 SVR 11.182533 4.675396 14.465358 14.465348 3.282825 9.789953 D     We can first say that even though some models have the same performance on the test sets for either the grouped or naive approach, the difference between cross valitaion error is much smaller for the grouped approach. This means that we can better estimate how a model will perform with unseen data by using the grouped approach and cluster data before training.\nFor example, using the results for mill A and xgb (1st line). The grouped CV mean absolute error was 12.3, the naive was 8.2. The actual error in the test set was the same for both but using a grouped cross validation leads to a much smaller difference between CVverror than using a naive approach (1.7 for grouped vs 5.8 for naive). The graph bellow gives us a better view of this pattern.\nConclusion If we were to predict each field\u0026rsquo;s yield using the average tonnes per hectare harvested, the mean absolute error would be vary between 17 to 18 tonnes per hectare, depending on the mill. By using these models, we might reduce (using xgb results as an example) these to 13-14 tonnes per hectare. That is a 18% decrease in error prediction. If a common sugarcane carrier truck can transport 15tonnes, this may be significant to take into consideration when planning harvest season.\nEven though it does not seem like a big improvement and besides using the mean we can also segment the mean by number of harvests and variety, etc. this dataset was acquired between 2010 and 2014. Since we\u0026rsquo;re in 2019 and data adequate data acquisition is more prioritized, modern datasets may yield better results. Another important point to note is that this approach is independent of the selected crop. We can use the same routine to predict soy, corn, cotton and many other main crop, given an adequately sized dataset.\n","date":1561403132,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561403132,"objectID":"2ef2572eb6f3eccbc982eeea04e97d6d","permalink":"https://mferrac.github.io/post/sugarcane-yield/","publishdate":"2019-06-24T16:05:32-03:00","relpermalink":"/post/sugarcane-yield/","section":"post","summary":"Yield modeling The harvesting operation involves transportation of heavy agricultural machinery across great distances in the fields. Effective management of field operations can improve adequate machine use planning, which, in turn can reduce costs related to fuel and workforce allocation.\nIf we can more accurately predict how much will be harvested in each field, we can have a better planning of the whole operation. In this post I\u0026rsquo;ll discuss how we can use available data and moder machine learning algorthims to improve harvest prediction.","tags":null,"title":"Sugarcane Yield","type":"post"},{"authors":null,"categories":null,"content":" Aula 05 Na aula de hoje, usaremos os dados e códigos da pasta da Aula 05\n1ª parte Na primeira parte da aula trabalharemos com dados sintéticos, denominados first, second e third data.\n2ª parte Na segunda parte, usaremos o conjunto de dados de cereais (cereals.csv). Com os atributos:\n Name : Name of cereal mfr : Manufacturer of cereal  A = American Home Food Products; G = General Mills K = Kelloggs N = Nabisco P = Post Q = Quaker Oats R = Ralston Purina  type:  cold hot  calories: calories per serving protein: grams of protein fat: grams of fat sodium: milligrams of sodium fiber: grams of dietary fiber carbo: grams of complex carbohydrates sugars: grams of sugars potass: milligrams of potassium vitamins: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended shelf: display shelf (1, 2, or 3, counting from the floor) weight: weight in ounces of one serving cups: number of cups in one serving rating: a rating of the cereals (Possibly from Consumer Reports?)  ","date":1558966914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558966914,"objectID":"8f1578550748cce3f6fb09e5cd5c183a","permalink":"https://mferrac.github.io/post/feg18-aula05/","publishdate":"2019-05-27T11:21:54-03:00","relpermalink":"/post/feg18-aula05/","section":"post","summary":"Aula 05 Na aula de hoje, usaremos os dados e códigos da pasta da Aula 05\n1ª parte Na primeira parte da aula trabalharemos com dados sintéticos, denominados first, second e third data.\n2ª parte Na segunda parte, usaremos o conjunto de dados de cereais (cereals.csv). Com os atributos:\n Name : Name of cereal mfr : Manufacturer of cereal  A = American Home Food Products; G = General Mills K = Kelloggs N = Nabisco P = Post Q = Quaker Oats R = Ralston Purina  type:  cold hot  calories: calories per serving protein: grams of protein fat: grams of fat sodium: milligrams of sodium fiber: grams of dietary fiber carbo: grams of complex carbohydrates sugars: grams of sugars potass: milligrams of potassium vitamins: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended shelf: display shelf (1, 2, or 3, counting from the floor) weight: weight in ounces of one serving cups: number of cups in one serving rating: a rating of the cereals (Possibly from Consumer Reports?","tags":null,"title":"Análise de dados - Aula 05","type":"post"},{"authors":null,"categories":[],"content":" Dados da aula Os dados podem ser baixados na pasta da Aula 04\nInstalações Para esta aula, serão necessários os pacotes:\n class: Implementação de knn para classificação FNN: Tem uma implementação de knn para regressão  ","date":1558364450,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558364450,"objectID":"70f8233c19583e186849ed1b5e28898e","permalink":"https://mferrac.github.io/post/feg18-aula04/","publishdate":"2019-05-20T12:00:50-03:00","relpermalink":"/post/feg18-aula04/","section":"post","summary":" Dados da aula Os dados podem ser baixados na pasta da Aula 04\nInstalações Para esta aula, serão necessários os pacotes:\n class: Implementação de knn para classificação FNN: Tem uma implementação de knn para regressão  ","tags":[],"title":"Análise de Dados - Aula 04","type":"post"},{"authors":null,"categories":[],"content":" Homework  Baixe os dados na pasta Homework da Aula 03\n Importe os dados no Rstudio\n Envie as respostas pelo formulário\n  Dados para a aula Baixar em https://transfer.sh/I109G/aula03.zip\nInstalações Para esta aula, serão necessários os pacotes:\n mlr: Esta bibloteca serve com interface para os algoritmos de machine learning usados no curso. Mais informações no site. rpart: A implementação do algoritmo de árvores de decisão. Mais informações no manual. rpart.plot: Contém funções para visualização da árvore de decisão. tidyverse: Instalado na aula 02.  Para instalar cada um destes pacotes, basta usar o comandos install.packages(\u0026quot;nome_do_pacote\u0026quot;). Por exemplo:\ninstall.packages(\u0026quot;mlr\u0026quot;) install.packages(\u0026quot;rpart\u0026quot;) install.packages(\u0026quot;rpart.plot\u0026quot;)  Para instalar todos os pacotes de uma vez, use:\ninstall.packages(c(\u0026quot;mlr\u0026quot;,\u0026quot;rpart\u0026quot;,\u0026quot;rpart.plot\u0026quot;))  ","date":1557688430,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557688430,"objectID":"38bf867ff387c1bf3580d3a859cf9428","permalink":"https://mferrac.github.io/post/feg18-aula03/","publishdate":"2019-05-12T16:13:50-03:00","relpermalink":"/post/feg18-aula03/","section":"post","summary":"Homework  Baixe os dados na pasta Homework da Aula 03\n Importe os dados no Rstudio\n Envie as respostas pelo formulário\n  Dados para a aula Baixar em https://transfer.sh/I109G/aula03.zip\nInstalações Para esta aula, serão necessários os pacotes:\n mlr: Esta bibloteca serve com interface para os algoritmos de machine learning usados no curso. Mais informações no site. rpart: A implementação do algoritmo de árvores de decisão. Mais informações no manual.","tags":[],"title":"Análise de Dados - Aula 03","type":"post"},{"authors":null,"categories":[],"content":" Homework  Baixe os dados em https://transfer.sh/ElXkS/adult_data.csv\n Importe os dados no Rstudio\n Envie as respostas pelo formulário\n  Arquivos da aula O notebook da aula pode ser baixado neste link\nOs arquivos da aula podem ser baixados neste link.\nInstalação do Jupyter com R Para instalar o Jupyter notebook no windows, primeiro é preciso instalar o anaconda no link:\nhttps://www.anaconda.com/distribution/\nSelecione \u0026ldquo;windows\u0026rdquo; e clique em Download abaixo de Python 3.7 Version.\nEm seguida, execute o arquivo Anaconda3-2019.03-Windows-x86_64.exe e siga as instruções de instalação.\nApós instalar, procure Anaconda Prompt na pesquisa do Windows e execute o programa.\nNo terminal aberto, digite o comando:\nconda install -c r r-irkernel  Básicos de R com swirl Para instalar o pacote swirl, basta executar o comando abaixo. Este comando instala o pacote na sua máquina. As aspas são obrigatórias. Uma vez instalado, não precisa executar de novo.\ninstall.packages('swirl')  Para carregar o pacote, use library(swirl) ou library('swirl'). Este comando ativa o pacote na sessão aberta do R e deve ser executado sempre que se reinicia o programa. As aspas são opcionais.\nPara começar o tutorial, após executar o comando acima, digite:\nswirl()  Em seguida, siga o tutorial que vai começar na tela. Coloque um nome para continuar de onde parou e manter o progresso de cada sessão. Recomendamos fazer as sessões de 1 a 4, no mínimo para a aula 02. Idealmente, façam quantas puderem, até a 8.\nÉ melhor fazer uma ou duas sessões por dia, do que todas de uma vez no final de semana.\nLembrem-se que para o swirl o correto é usar \u0026lt;- no lugar de =.\n","date":1556713970,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556713970,"objectID":"cf43c7d197c9d9227d547a8c3c5e5f11","permalink":"https://mferrac.github.io/post/feg18-aula02/","publishdate":"2019-05-01T09:32:50-03:00","relpermalink":"/post/feg18-aula02/","section":"post","summary":"Homework  Baixe os dados em https://transfer.sh/ElXkS/adult_data.csv\n Importe os dados no Rstudio\n Envie as respostas pelo formulário\n  Arquivos da aula O notebook da aula pode ser baixado neste link\nOs arquivos da aula podem ser baixados neste link.\nInstalação do Jupyter com R Para instalar o Jupyter notebook no windows, primeiro é preciso instalar o anaconda no link:\nhttps://www.anaconda.com/distribution/\nSelecione \u0026ldquo;windows\u0026rdquo; e clique em Download abaixo de Python 3.","tags":[],"title":"Análise de Dados - Aula 02","type":"post"}]